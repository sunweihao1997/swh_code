{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåÄ Improved Typhoon Prediction Model v4.1\n",
        "\n",
        "## Fixes for Flat Prediction Problem\n",
        "\n",
        "This notebook addresses the **flat prediction issue** where models output near-constant values with minimal variance.\n",
        "\n",
        "### Key Improvements:\n",
        "1. **Feature Selection** - Reduce from 8 to 3-4 most predictive features\n",
        "2. **Lower SVR Epsilon** - Force model to capture variance (0.001-0.1 vs 0.01-0.5)\n",
        "3. **Leave-One-Out CV** - Better validation for small datasets\n",
        "4. **Bayesian Models** - BayesianRidge for uncertainty quantification\n",
        "5. **Lagged Features** - Capture longer-term climate patterns\n",
        "6. **Diagnostic Tools** - Monitor prediction variance\n",
        "\n",
        "---\n",
        "‚è±Ô∏è **Estimated Runtime**: 5-15 minutes"
      ],
      "metadata": {
        "id": "header_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Environment Setup"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from itertools import product\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern, WhiteKernel\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import Ridge, PoissonRegressor, ElasticNet, BayesianRidge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, LeaveOneOut, cross_val_predict\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n",
        "print(\"\\nüìä Key Changes from Original:\")\n",
        "print(\"   ‚Ä¢ Added BayesianRidge, ElasticNet models\")\n",
        "print(\"   ‚Ä¢ Added feature selection (SelectKBest)\")\n",
        "print(\"   ‚Ä¢ Added Leave-One-Out cross-validation\")\n",
        "print(\"   ‚Ä¢ Lower epsilon values for SVR\")"
      ],
      "metadata": {
        "id": "setup_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Upload Data Files"
      ],
      "metadata": {
        "id": "upload_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"Please upload: typhoon_count.csv and MEI/PDO/IOD/QBO NC files\")\n",
        "uploaded = files.upload()\n",
        "print(f\"\\n‚úÖ Uploaded {len(uploaded)} files\")"
      ],
      "metadata": {
        "id": "upload_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Check NC Files"
      ],
      "metadata": {
        "id": "check_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for f in [f for f in os.listdir('.') if f.endswith('.nc')]:\n",
        "    ds = xr.open_dataset(f)\n",
        "    print(f\"{f}: variables={list(ds.data_vars)}\")\n",
        "    ds.close()"
      ],
      "metadata": {
        "id": "check_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Configuration ‚ö†Ô∏è IMPROVED\n",
        "\n",
        "### Key Changes:\n",
        "- **SVR epsilon**: Changed from `[0.01, 0.1, 0.5]` to `[0.001, 0.01, 0.05, 0.1]`\n",
        "- **Feature selection**: Enabled with max 4 features\n",
        "- **Leave-One-Out CV**: Enabled for small dataset\n",
        "- **Added**: BayesianRidge, ElasticNet models\n",
        "- **Added**: Lagged features (1-2 year lags)"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Data files - Update if needed\n",
        "    TYPHOON_DATA_PATH = 'typhoon_count.csv'\n",
        "    MEI_NC_PATH = 'mei_v2.nc'\n",
        "    PDO_NC_PATH = 'pdo_ersst_v5.nc'\n",
        "    IOD_NC_PATH = 'iod_ersst_v5.nc'\n",
        "    QBO_NC_PATH = 'qbo.nc'\n",
        "\n",
        "    # NC variable names\n",
        "    MEI_VAR_NAME = 'mei'\n",
        "    PDO_VAR_NAME = 'pdo'\n",
        "    IOD_VAR_NAME = 'iod'\n",
        "    QBO_VAR_NAME = 'value'\n",
        "    TIME_VAR_NAME = 'time'\n",
        "\n",
        "    # Time range\n",
        "    START_YEAR = 1980\n",
        "    END_YEAR = 2024\n",
        "    PREDICT_YEAR = 2025\n",
        "    TEST_SPLIT_YEAR = 2015\n",
        "\n",
        "    # ========== KEY FIXES ==========\n",
        "    # Use Leave-One-Out CV for small datasets (37 samples)\n",
        "    USE_LOO_CV = True\n",
        "    CV_N_SPLITS = 5  # Fallback if LOO too slow\n",
        "\n",
        "    # Feature selection - CRITICAL for small sample size\n",
        "    USE_FEATURE_SELECTION = True\n",
        "    MAX_FEATURES = 4  # Reduce from 8 to 4 features\n",
        "\n",
        "    # Add lagged features for better signal\n",
        "    USE_LAGGED_FEATURES = True\n",
        "    LAG_YEARS = [1, 2]\n",
        "\n",
        "    # Models optimized for small samples\n",
        "    MODELS_TO_USE = ['SVR', 'BayesianRidge', 'KNN', 'Ridge', 'ElasticNet', 'GaussianProcess']\n",
        "\n",
        "    # ========== IMPROVED HYPERPARAMETER GRIDS ==========\n",
        "    PARAM_GRIDS = {\n",
        "        'SVR': {\n",
        "            # CRITICAL FIX: Much smaller epsilon values!\n",
        "            'C': [0.1, 1, 10, 50],\n",
        "            'epsilon': [0.001, 0.01, 0.05, 0.1],  # Was [0.01, 0.1, 0.5]\n",
        "            'kernel': ['rbf', 'linear'],\n",
        "            'gamma': ['scale', 0.001, 0.01, 0.1],\n",
        "        },\n",
        "        'BayesianRidge': {\n",
        "            'alpha_1': [1e-6, 1e-5, 1e-4],\n",
        "            'alpha_2': [1e-6, 1e-5, 1e-4],\n",
        "            'lambda_1': [1e-6, 1e-5, 1e-4],\n",
        "            'lambda_2': [1e-6, 1e-5, 1e-4],\n",
        "        },\n",
        "        'GaussianProcess': {\n",
        "            'alpha': [0.1, 0.5, 1.0],  # Higher alpha for noisy small data\n",
        "            'kernel_type': ['RBF', 'Matern'],\n",
        "            'length_scale': [1.0, 2.0, 5.0],\n",
        "        },\n",
        "        'KNN': {\n",
        "            'n_neighbors': [3, 5, 7],\n",
        "            'weights': ['distance'],  # Distance weighting helps\n",
        "            'metric': ['euclidean', 'manhattan'],\n",
        "        },\n",
        "        'Ridge': {\n",
        "            'alpha': [0.1, 1.0, 10.0, 100.0],\n",
        "            'solver': ['svd', 'lsqr'],\n",
        "        },\n",
        "        'ElasticNet': {\n",
        "            'alpha': [0.01, 0.1, 1.0],\n",
        "            'l1_ratio': [0.2, 0.5, 0.8],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    REGIONS = ['South China Sea', 'Eastern China Sea', 'Japan Sea', 'Yellow Sea']\n",
        "    USE_TREND_FEATURES = True\n",
        "    OUTPUT_DIR = 'model_outputs_improved'\n",
        "\n",
        "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "print(\"‚úÖ Configuration complete\")\n",
        "print(f\"   Models: {Config.MODELS_TO_USE}\")\n",
        "print(f\"   Feature Selection: {Config.USE_FEATURE_SELECTION} (max {Config.MAX_FEATURES} features)\")\n",
        "print(f\"   Leave-One-Out CV: {Config.USE_LOO_CV}\")\n",
        "print(f\"   SVR epsilon range: {Config.PARAM_GRIDS['SVR']['epsilon']}\")"
      ],
      "metadata": {
        "id": "config_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Data Loader with Enhanced Features"
      ],
      "metadata": {
        "id": "loader_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.typhoon_data = None\n",
        "        self.climate_indices = None\n",
        "\n",
        "    def load_typhoon_data(self):\n",
        "        print(\"Loading typhoon data...\")\n",
        "        self.typhoon_data = pd.read_csv(self.config.TYPHOON_DATA_PATH)\n",
        "        print(f\"  ‚úì {len(self.typhoon_data)} records\")\n",
        "        return self.typhoon_data\n",
        "\n",
        "    def load_climate_index(self, path, var_name, name):\n",
        "        try:\n",
        "            ds = xr.open_dataset(path)\n",
        "            if var_name not in ds.data_vars:\n",
        "                var_name = list(ds.data_vars)[0]\n",
        "            df = ds[var_name].to_dataframe().reset_index()\n",
        "            df['year'] = pd.to_datetime(df['time']).dt.year\n",
        "            df['month'] = pd.to_datetime(df['time']).dt.month\n",
        "            df['value'] = df[var_name]\n",
        "            ds.close()\n",
        "            return df[['year', 'month', 'value']]\n",
        "        except Exception as e:\n",
        "            print(f\"  ! {name} load failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_all_climate_indices(self):\n",
        "        print(\"Loading climate indices...\")\n",
        "        self.climate_indices = {}\n",
        "        for name, path, var in [\n",
        "            ('MEI', self.config.MEI_NC_PATH, self.config.MEI_VAR_NAME),\n",
        "            ('PDO', self.config.PDO_NC_PATH, self.config.PDO_VAR_NAME),\n",
        "            ('IOD', self.config.IOD_NC_PATH, self.config.IOD_VAR_NAME),\n",
        "            ('QBO', self.config.QBO_NC_PATH, self.config.QBO_VAR_NAME)\n",
        "        ]:\n",
        "            data = self.load_climate_index(path, var, name)\n",
        "            if data is not None:\n",
        "                self.climate_indices[name] = data\n",
        "                print(f\"  ‚úì {name}\")\n",
        "        return self.climate_indices\n",
        "\n",
        "    def calc_avg(self, data, year, months):\n",
        "        mask = (data['year'] == year) & (data['month'].isin(months))\n",
        "        vals = data.loc[mask, 'value']\n",
        "        return vals.mean() if len(vals) >= 2 else np.nan\n",
        "\n",
        "    def calc_std(self, data, year, months):\n",
        "        \"\"\"Calculate standard deviation for variability features\"\"\"\n",
        "        mask = (data['year'] == year) & (data['month'].isin(months))\n",
        "        vals = data.loc[mask, 'value']\n",
        "        return vals.std() if len(vals) >= 2 else np.nan\n",
        "\n",
        "    def build_feature_matrix(self):\n",
        "        \"\"\"Build feature matrix with ENHANCED feature engineering\"\"\"\n",
        "        print(\"Building feature matrix with enhanced features...\")\n",
        "        years = [y for y in self.typhoon_data['Year'].unique()\n",
        "                 if self.config.START_YEAR <= y <= self.config.END_YEAR]\n",
        "\n",
        "        records = []\n",
        "        for year in sorted(years):\n",
        "            rec = {'Year': year}\n",
        "            prev = year - 1\n",
        "\n",
        "            for idx, data in self.climate_indices.items():\n",
        "                # Oct-Nov-Dec average (original)\n",
        "                ond = self.calc_avg(data, prev, [10, 11, 12])\n",
        "                rec[f'{idx}_OND'] = ond\n",
        "\n",
        "                # Trend feature (original)\n",
        "                if self.config.USE_TREND_FEATURES:\n",
        "                    jas = self.calc_avg(data, prev, [7, 8, 9])\n",
        "                    rec[f'{idx}_TREND'] = ond - jas if not np.isnan(ond) and not np.isnan(jas) else np.nan\n",
        "\n",
        "                # NEW: Annual mean\n",
        "                annual = self.calc_avg(data, prev, list(range(1, 13)))\n",
        "                rec[f'{idx}_ANNUAL'] = annual\n",
        "\n",
        "            # NEW: Lagged features (1-2 year lags)\n",
        "            if self.config.USE_LAGGED_FEATURES:\n",
        "                for lag in self.config.LAG_YEARS:\n",
        "                    lag_year = year - 1 - lag\n",
        "                    for idx, data in self.climate_indices.items():\n",
        "                        lag_ond = self.calc_avg(data, lag_year, [10, 11, 12])\n",
        "                        rec[f'{idx}_OND_LAG{lag}'] = lag_ond\n",
        "\n",
        "            # Current year ENSO (for seasonal prediction)\n",
        "            if 'MEI' in self.climate_indices:\n",
        "                rec['MEI_Current_JASO'] = self.calc_avg(\n",
        "                    self.climate_indices['MEI'], year, [7, 8, 9, 10]\n",
        "                )\n",
        "\n",
        "            records.append(rec)\n",
        "\n",
        "        df = pd.DataFrame(records)\n",
        "\n",
        "        # Pivot typhoon counts\n",
        "        pivot = self.typhoon_data.pivot_table(\n",
        "            index='Year', columns='Region',\n",
        "            values='Typhoon_Count', aggfunc='sum'\n",
        "        ).reset_index()\n",
        "\n",
        "        for col in pivot.columns:\n",
        "            if col != 'Year':\n",
        "                pivot = pivot.rename(columns={col: f'Target_{col}'})\n",
        "\n",
        "        result = df.merge(pivot, on='Year').dropna()\n",
        "        feature_count = len([c for c in result.columns if c not in ['Year'] and not c.startswith('Target')])\n",
        "        print(f\"  ‚úì {len(result)} samples with {feature_count} features\")\n",
        "\n",
        "        return result\n",
        "\n",
        "print(\"‚úÖ Data loader module defined\")"
      ],
      "metadata": {
        "id": "loader_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Improved Model Factory"
      ],
      "metadata": {
        "id": "factory_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelFactory:\n",
        "    @staticmethod\n",
        "    def create(name, params):\n",
        "        if name == 'SVR':\n",
        "            return SVR(\n",
        "                C=params.get('C', 1),\n",
        "                epsilon=params.get('epsilon', 0.01),  # Lower default!\n",
        "                kernel=params.get('kernel', 'rbf'),\n",
        "                gamma=params.get('gamma', 'scale')\n",
        "            )\n",
        "        elif name == 'BayesianRidge':\n",
        "            return BayesianRidge(\n",
        "                alpha_1=params.get('alpha_1', 1e-6),\n",
        "                alpha_2=params.get('alpha_2', 1e-6),\n",
        "                lambda_1=params.get('lambda_1', 1e-6),\n",
        "                lambda_2=params.get('lambda_2', 1e-6),\n",
        "            )\n",
        "        elif name == 'GaussianProcess':\n",
        "            kt = params.get('kernel_type', 'RBF')\n",
        "            ls = params.get('length_scale', 1.0)\n",
        "            if kt == 'RBF':\n",
        "                kernel = ConstantKernel(1.0) * RBF(ls) + WhiteKernel(noise_level=0.1)\n",
        "            else:\n",
        "                kernel = ConstantKernel(1.0) * Matern(ls, nu=1.5) + WhiteKernel(noise_level=0.1)\n",
        "            return GaussianProcessRegressor(\n",
        "                kernel=kernel,\n",
        "                alpha=params.get('alpha', 0.5),\n",
        "                random_state=42,\n",
        "                normalize_y=True\n",
        "            )\n",
        "        elif name == 'KNN':\n",
        "            return KNeighborsRegressor(\n",
        "                n_neighbors=params.get('n_neighbors', 5),\n",
        "                weights=params.get('weights', 'distance'),\n",
        "                metric=params.get('metric', 'euclidean')\n",
        "            )\n",
        "        elif name == 'Ridge':\n",
        "            return Ridge(alpha=params.get('alpha', 1.0))\n",
        "        elif name == 'ElasticNet':\n",
        "            return ElasticNet(\n",
        "                alpha=params.get('alpha', 0.1),\n",
        "                l1_ratio=params.get('l1_ratio', 0.5),\n",
        "                max_iter=2000\n",
        "            )\n",
        "        elif name == 'RandomForest':\n",
        "            return RandomForestRegressor(\n",
        "                n_estimators=params.get('n_estimators', 100),\n",
        "                max_depth=params.get('max_depth', 3),\n",
        "                min_samples_split=params.get('min_samples_split', 5),\n",
        "                random_state=42\n",
        "            )\n",
        "        elif name == 'Poisson':\n",
        "            return PoissonRegressor(\n",
        "                alpha=params.get('alpha', 1.0),\n",
        "                max_iter=1000\n",
        "            )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_param_combos(grid):\n",
        "        if not grid:\n",
        "            return [{}]\n",
        "        keys, vals = list(grid.keys()), list(grid.values())\n",
        "        return [dict(zip(keys, v)) for v in product(*vals)]\n",
        "\n",
        "print(\"‚úÖ Model factory defined\")"
      ],
      "metadata": {
        "id": "factory_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Improved Multi-Model System\n",
        "\n",
        "### Key Improvements:\n",
        "- **Feature Selection**: Uses mutual information to select top features\n",
        "- **Leave-One-Out CV**: More reliable for small datasets\n",
        "- **Variance Monitoring**: Tracks if predictions are too flat\n",
        "- **Diagnostic Output**: Warns about flat predictions"
      ],
      "metadata": {
        "id": "system_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModelSystem:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.results = {}\n",
        "        self.best_models = {}\n",
        "        self.trained = {}\n",
        "        self.scalers = {}\n",
        "        self.feature_selectors = {}\n",
        "        self.selected_features = {}\n",
        "        self.correction = {}\n",
        "        self.feature_cols = None\n",
        "\n",
        "    def select_features(self, X, y, feature_names, max_features=4):\n",
        "        \"\"\"Select most predictive features using mutual information\"\"\"\n",
        "        k = min(max_features, X.shape[1])\n",
        "        selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
        "        selector.fit(X, y)\n",
        "        mask = selector.get_support()\n",
        "        selected = [f for f, m in zip(feature_names, mask) if m]\n",
        "        scores = dict(zip(feature_names, selector.scores_))\n",
        "        return selector, selected, scores\n",
        "\n",
        "    def cv_with_loo(self, X, y, model_name, params):\n",
        "        \"\"\"Leave-One-Out cross-validation for small datasets\"\"\"\n",
        "        loo = LeaveOneOut()\n",
        "        predictions = []\n",
        "        actuals = []\n",
        "\n",
        "        for train_idx, test_idx in loo.split(X):\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_train_s = scaler.fit_transform(X_train)\n",
        "            X_test_s = scaler.transform(X_test)\n",
        "\n",
        "            y_train_fit = np.maximum(y_train, 0.1) if model_name == 'Poisson' else y_train\n",
        "\n",
        "            try:\n",
        "                model = ModelFactory.create(model_name, params)\n",
        "                model.fit(X_train_s, y_train_fit)\n",
        "                pred = np.maximum(model.predict(X_test_s), 0)[0]\n",
        "                predictions.append(pred)\n",
        "                actuals.append(y_test[0])\n",
        "            except Exception as e:\n",
        "                return 999, 999\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        actuals = np.array(actuals)\n",
        "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
        "        mae = mean_absolute_error(actuals, predictions)\n",
        "\n",
        "        return rmse, mae\n",
        "\n",
        "    def cv_with_tscv(self, X, y, model_name, params, n_splits=5):\n",
        "        \"\"\"Time series cross-validation fallback\"\"\"\n",
        "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "        rmses = []\n",
        "\n",
        "        for train_idx, val_idx in tscv.split(X):\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X[train_idx])\n",
        "            X_val = scaler.transform(X[val_idx])\n",
        "            y_train = np.maximum(y[train_idx], 0.1) if model_name == 'Poisson' else y[train_idx]\n",
        "\n",
        "            try:\n",
        "                model = ModelFactory.create(model_name, params)\n",
        "                model.fit(X_train, y_train)\n",
        "                pred = np.maximum(model.predict(X_val), 0)\n",
        "                rmses.append(np.sqrt(mean_squared_error(y[val_idx], pred)))\n",
        "            except:\n",
        "                rmses.append(999)\n",
        "\n",
        "        return np.mean(rmses), np.std(rmses)\n",
        "\n",
        "    def search_best_params(self, X, y, model_name, grid):\n",
        "        \"\"\"Search for best hyperparameters\"\"\"\n",
        "        combos = ModelFactory.get_param_combos(grid)\n",
        "        best_rmse = float('inf')\n",
        "        best_params = {}\n",
        "\n",
        "        for params in combos:\n",
        "            if self.config.USE_LOO_CV:\n",
        "                rmse, _ = self.cv_with_loo(X, y, model_name, params)\n",
        "            else:\n",
        "                rmse, _ = self.cv_with_tscv(X, y, model_name, params, self.config.CV_N_SPLITS)\n",
        "\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_params = params\n",
        "\n",
        "        return best_params, best_rmse\n",
        "\n",
        "    def train_region(self, data, region, feat_cols):\n",
        "        \"\"\"Train models for a specific region with improvements\"\"\"\n",
        "        target = f'Target_{region}'\n",
        "        if target not in data.columns:\n",
        "            print(f\"  ! Target column {target} not found\")\n",
        "            return None\n",
        "\n",
        "        X = data[feat_cols].values\n",
        "        y = data[target].values\n",
        "        years = data['Year'].values\n",
        "\n",
        "        train_mask = years < self.config.TEST_SPLIT_YEAR\n",
        "        test_mask = years >= self.config.TEST_SPLIT_YEAR\n",
        "\n",
        "        X_train, X_test = X[train_mask], X[test_mask]\n",
        "        y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "        print(f\"\\n  === {region} ===\")\n",
        "        print(f\"      Training: {len(y_train)} samples, Test: {len(y_test)} samples\")\n",
        "        print(f\"      Target mean: {y_train.mean():.1f}, std: {y_train.std():.1f}\")\n",
        "\n",
        "        # Feature selection\n",
        "        if self.config.USE_FEATURE_SELECTION and len(feat_cols) > self.config.MAX_FEATURES:\n",
        "            selector, selected, scores = self.select_features(\n",
        "                X_train, y_train, feat_cols, self.config.MAX_FEATURES\n",
        "            )\n",
        "            self.feature_selectors[region] = selector\n",
        "            self.selected_features[region] = selected\n",
        "            X_train = selector.transform(X_train)\n",
        "            X_test = selector.transform(X_test)\n",
        "            print(f\"      Selected features: {selected}\")\n",
        "        else:\n",
        "            self.selected_features[region] = feat_cols\n",
        "\n",
        "        # Scale\n",
        "        scaler = StandardScaler()\n",
        "        X_train_s = scaler.fit_transform(X_train)\n",
        "        X_test_s = scaler.transform(X_test)\n",
        "        self.scalers[region] = scaler\n",
        "\n",
        "        # Search for best model\n",
        "        model_results = {}\n",
        "        for model_name in self.config.MODELS_TO_USE:\n",
        "            print(f\"    {model_name}...\", end=\" \")\n",
        "            grid = self.config.PARAM_GRIDS.get(model_name, {})\n",
        "\n",
        "            best_params, cv_rmse = self.search_best_params(X_train, y_train, model_name, grid)\n",
        "            print(f\"CV RMSE={cv_rmse:.2f}\")\n",
        "\n",
        "            model_results[model_name] = {\n",
        "                'params': best_params,\n",
        "                'cv_rmse': cv_rmse\n",
        "            }\n",
        "\n",
        "        # Select best model\n",
        "        best_model_name = min(model_results, key=lambda m: model_results[m]['cv_rmse'])\n",
        "        best_params = model_results[best_model_name]['params']\n",
        "        print(f\"  üèÜ Best model: {best_model_name}\")\n",
        "        if best_model_name == 'SVR':\n",
        "            print(f\"      SVR epsilon: {best_params.get('epsilon', 'default')}\")\n",
        "\n",
        "        # Train final model\n",
        "        y_train_fit = np.maximum(y_train, 0.1) if best_model_name == 'Poisson' else y_train\n",
        "        final_model = ModelFactory.create(best_model_name, best_params)\n",
        "        final_model.fit(X_train_s, y_train_fit)\n",
        "\n",
        "        # Predictions\n",
        "        y_train_pred = np.maximum(final_model.predict(X_train_s), 0)\n",
        "        y_test_pred = np.maximum(final_model.predict(X_test_s), 0)\n",
        "\n",
        "        # Metrics\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "        test_r2 = r2_score(y_test, y_test_pred)\n",
        "        train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "        # Check for flat predictions (DIAGNOSTIC)\n",
        "        pred_std = np.std(y_test_pred)\n",
        "        actual_std = np.std(y_test)\n",
        "        variance_ratio = pred_std / actual_std if actual_std > 0 else 0\n",
        "\n",
        "        print(f\"     Train R¬≤={train_r2:.2f}, Test RMSE={test_rmse:.2f}, Test R¬≤={test_r2:.2f}\")\n",
        "        print(f\"     Prediction std={pred_std:.2f}, Actual std={actual_std:.2f}, Variance ratio={variance_ratio:.2f}\")\n",
        "\n",
        "        if variance_ratio < 0.3:\n",
        "            print(f\"     ‚ö†Ô∏è WARNING: Predictions still too flat!\")\n",
        "        elif variance_ratio < 0.6:\n",
        "            print(f\"     ‚ö†Ô∏è CAUTION: Predictions under-capture variance\")\n",
        "        else:\n",
        "            print(f\"     ‚úì Variance capture looks reasonable\")\n",
        "\n",
        "        # Store results\n",
        "        self.best_models[region] = best_model_name\n",
        "        self.trained[region] = {best_model_name: final_model}\n",
        "\n",
        "        # Train all models for comparison\n",
        "        for mn in self.config.MODELS_TO_USE:\n",
        "            if mn not in self.trained[region]:\n",
        "                try:\n",
        "                    yt = np.maximum(y_train, 0.1) if mn == 'Poisson' else y_train\n",
        "                    m = ModelFactory.create(mn, model_results[mn]['params'])\n",
        "                    m.fit(X_train_s, yt)\n",
        "                    self.trained[region][mn] = m\n",
        "                    pred = np.maximum(m.predict(X_test_s), 0)\n",
        "                    model_results[mn]['test_rmse'] = np.sqrt(mean_squared_error(y_test, pred))\n",
        "                    model_results[mn]['test_r2'] = r2_score(y_test, pred)\n",
        "                except:\n",
        "                    model_results[mn]['test_rmse'] = 999\n",
        "                    model_results[mn]['test_r2'] = -999\n",
        "\n",
        "        model_results[best_model_name]['test_rmse'] = test_rmse\n",
        "        model_results[best_model_name]['test_r2'] = test_r2\n",
        "\n",
        "        # ENSO correction\n",
        "        if 'MEI_Current_JASO' in data.columns:\n",
        "            mei = data.loc[train_mask, 'MEI_Current_JASO'].values\n",
        "            res = y_train - y_train_pred\n",
        "            valid = ~np.isnan(mei)\n",
        "            if valid.sum() > 5:\n",
        "                slope, inter = np.polyfit(mei[valid], res[valid], 1)\n",
        "                self.correction[region] = {'slope': slope, 'intercept': inter}\n",
        "\n",
        "        self.results[region] = {\n",
        "            'models': model_results,\n",
        "            'best': best_model_name,\n",
        "            'best_params': best_params,\n",
        "            'test_rmse': test_rmse,\n",
        "            'test_r2': test_r2,\n",
        "            'train_r2': train_r2,\n",
        "            'y_train': y_train,\n",
        "            'y_train_pred': y_train_pred,\n",
        "            'y_test': y_test,\n",
        "            'y_test_pred': y_test_pred,\n",
        "            'years_train': years[train_mask],\n",
        "            'years_test': years[test_mask],\n",
        "            'pred_std': pred_std,\n",
        "            'variance_ratio': variance_ratio,\n",
        "        }\n",
        "\n",
        "    def train_all(self, data):\n",
        "        \"\"\"Train models for all regions\"\"\"\n",
        "        self.feature_cols = [c for c in data.columns\n",
        "                           if c not in ['Year'] and not c.startswith('Target')]\n",
        "\n",
        "        print(f\"\\nTotal available features: {len(self.feature_cols)}\")\n",
        "        print(f\"Features: {self.feature_cols}\")\n",
        "\n",
        "        for region in self.config.REGIONS:\n",
        "            self.train_region(data, region, self.feature_cols)\n",
        "\n",
        "    def predict(self, features, enso=None):\n",
        "        \"\"\"Make predictions for new data\"\"\"\n",
        "        preds = {}\n",
        "        for region, mn in self.best_models.items():\n",
        "            # Get feature values\n",
        "            if region in self.feature_selectors:\n",
        "                X = np.array([[features[c] for c in self.feature_cols]])\n",
        "                X = self.feature_selectors[region].transform(X)\n",
        "            else:\n",
        "                selected = self.selected_features[region]\n",
        "                X = np.array([[features[c] for c in selected]])\n",
        "\n",
        "            Xs = self.scalers[region].transform(X)\n",
        "            base = float(np.maximum(self.trained[region][mn].predict(Xs), 0)[0])\n",
        "\n",
        "            corr = 0\n",
        "            if enso and region in self.correction:\n",
        "                corr = self.correction[region]['slope'] * enso + self.correction[region]['intercept']\n",
        "\n",
        "            final = max(0, base + corr)\n",
        "            preds[region] = {'model': mn, 'base': base, 'corr': corr, 'final': round(final)}\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def comparison_table(self):\n",
        "        \"\"\"Generate comparison table\"\"\"\n",
        "        rows = []\n",
        "        for region, res in self.results.items():\n",
        "            for mn, mr in res['models'].items():\n",
        "                rows.append({\n",
        "                    'Region': region,\n",
        "                    'Model': mn,\n",
        "                    'CV_RMSE': mr['cv_rmse'],\n",
        "                    'Test_RMSE': mr.get('test_rmse', np.nan),\n",
        "                    'Test_R2': mr.get('test_r2', np.nan),\n",
        "                    'Best': '‚òÖ' if mn == res['best'] else ''\n",
        "                })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "print(\"‚úÖ Multi-model system defined\")"
      ],
      "metadata": {
        "id": "system_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Load Data and Build Features"
      ],
      "metadata": {
        "id": "load_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "loader = DataLoader(Config)\n",
        "loader.load_typhoon_data()\n",
        "loader.load_all_climate_indices()\n",
        "data = loader.build_feature_matrix()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Sample data:\")\n",
        "display(data.head())\n",
        "\n",
        "print(f\"\\nüìä Dataset Summary:\")\n",
        "print(f\"   Total samples: {len(data)}\")\n",
        "print(f\"   Training samples (< {Config.TEST_SPLIT_YEAR}): {len(data[data['Year'] < Config.TEST_SPLIT_YEAR])}\")\n",
        "print(f\"   Test samples (>= {Config.TEST_SPLIT_YEAR}): {len(data[data['Year'] >= Config.TEST_SPLIT_YEAR])}\")"
      ],
      "metadata": {
        "id": "load_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Train Models"
      ],
      "metadata": {
        "id": "train_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚è≥ Starting training (this may take several minutes)...\")\n",
        "print(\"   Using Leave-One-Out CV for reliable estimates on small data\\n\")\n",
        "\n",
        "system = MultiModelSystem(Config)\n",
        "system.train_all(data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Training complete!\")"
      ],
      "metadata": {
        "id": "train_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü Results Analysis"
      ],
      "metadata": {
        "id": "results_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model comparison table\n",
        "comparison = system.comparison_table()\n",
        "print(\"\\nüìä Model Comparison by Region:\")\n",
        "display(comparison)\n",
        "\n",
        "# Save results\n",
        "comparison.to_csv(f'{Config.OUTPUT_DIR}/model_comparison_improved.csv', index=False)\n",
        "print(f\"\\n‚úì Results saved to {Config.OUTPUT_DIR}/\")"
      ],
      "metadata": {
        "id": "results_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Visualization"
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs predicted\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (region, res) in enumerate(system.results.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Combine train and test\n",
        "    all_years = np.concatenate([res['years_train'], res['years_test']])\n",
        "    all_actual = np.concatenate([res['y_train'], res['y_test']])\n",
        "    all_pred = np.concatenate([res['y_train_pred'], res['y_test_pred']])\n",
        "\n",
        "    # Plot actual\n",
        "    ax.plot(all_years, all_actual, 'b-o', label='Actual', linewidth=2, markersize=4)\n",
        "\n",
        "    # Plot predictions\n",
        "    ax.plot(res['years_train'], res['y_train_pred'], 'g--s',\n",
        "           label='Train Pred', linewidth=1.5, markersize=3, alpha=0.7)\n",
        "    ax.plot(res['years_test'], res['y_test_pred'], 'r--^',\n",
        "           label='Test Pred', linewidth=2, markersize=5)\n",
        "\n",
        "    # Add train/test split line\n",
        "    ax.axvline(x=Config.TEST_SPLIT_YEAR - 0.5, color='gray',\n",
        "              linestyle=':', linewidth=2, label='Train/Test Split')\n",
        "\n",
        "    # Add mean line for reference\n",
        "    mean_val = np.mean(all_actual)\n",
        "    ax.axhline(y=mean_val, color='orange', linestyle='--',\n",
        "              alpha=0.5, label=f'Mean ({mean_val:.1f})')\n",
        "\n",
        "    ax.set_xlabel('Year')\n",
        "    ax.set_ylabel('Typhoon Count')\n",
        "    ax.set_title(f'{region}\\nBest: {res[\"best\"]} | R¬≤={res[\"test_r2\"]:.2f} | Var.Ratio={res[\"variance_ratio\"]:.2f}')\n",
        "    ax.legend(loc='upper right', fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{Config.OUTPUT_DIR}/predictions_improved.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Plot saved to {Config.OUTPUT_DIR}/predictions_improved.png\")"
      ],
      "metadata": {
        "id": "viz_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Diagnostic Summary"
      ],
      "metadata": {
        "id": "diag_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üìä DIAGNOSTIC SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for region, res in system.results.items():\n",
        "    print(f\"\\nüåä {region}:\")\n",
        "    print(f\"   Best Model: {res['best']}\")\n",
        "    print(f\"   Parameters: {res['best_params']}\")\n",
        "    print(f\"   Train R¬≤: {res['train_r2']:.3f}\")\n",
        "    print(f\"   Test R¬≤: {res['test_r2']:.3f}\")\n",
        "    print(f\"   Test RMSE: {res['test_rmse']:.3f}\")\n",
        "    print(f\"   Prediction Variance Ratio: {res['variance_ratio']:.3f}\")\n",
        "\n",
        "    if res['variance_ratio'] < 0.3:\n",
        "        print(f\"   ‚ö†Ô∏è ISSUE: Predictions still too flat!\")\n",
        "        print(f\"      Consider: More data, different features, or simpler baseline model\")\n",
        "    elif res['test_r2'] < 0:\n",
        "        print(f\"   ‚ö†Ô∏è ISSUE: Model worse than mean prediction!\")\n",
        "    else:\n",
        "        print(f\"   ‚úì Model shows reasonable performance\")\n",
        "\n",
        "    if region in system.selected_features:\n",
        "        print(f\"   Selected Features: {system.selected_features[region]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: Original vs Improved\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "Original Issues:\n",
        "  ‚Ä¢ SVR epsilon: [0.01, 0.1, 0.5] ‚Üí too high, ignored variance\n",
        "  ‚Ä¢ 8 features for 37 samples ‚Üí overfitting\n",
        "  ‚Ä¢ Negative R¬≤ values ‚Üí worse than mean prediction\n",
        "\n",
        "Improvements Applied:\n",
        "  ‚Ä¢ SVR epsilon: [0.001, 0.01, 0.05, 0.1] ‚Üí captures variance\n",
        "  ‚Ä¢ Feature selection: 4 best features ‚Üí better ratio\n",
        "  ‚Ä¢ Leave-One-Out CV ‚Üí reliable for small data\n",
        "  ‚Ä¢ BayesianRidge ‚Üí handles uncertainty better\n",
        "  ‚Ä¢ Lagged features ‚Üí more signal\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "diag_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£3Ô∏è‚É£ Make 2025 Prediction (Optional)"
      ],
      "metadata": {
        "id": "predict_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get latest feature values for 2025 prediction\n",
        "# This uses the most recent year's data as proxy\n",
        "\n",
        "latest_year = data['Year'].max()\n",
        "latest_row = data[data['Year'] == latest_year].iloc[0]\n",
        "\n",
        "# Build feature dict\n",
        "features_2025 = {}\n",
        "for col in system.feature_cols:\n",
        "    if col in latest_row:\n",
        "        features_2025[col] = latest_row[col]\n",
        "\n",
        "# Get ENSO value if available\n",
        "enso_val = latest_row.get('MEI_Current_JASO', None)\n",
        "\n",
        "print(f\"\\nüåÄ Predictions for {Config.PREDICT_YEAR}:\")\n",
        "print(f\"   (Using {latest_year} climate features as proxy)\\n\")\n",
        "\n",
        "predictions = system.predict(features_2025, enso=enso_val)\n",
        "\n",
        "for region, pred in predictions.items():\n",
        "    print(f\"   {region}:\")\n",
        "    print(f\"      Model: {pred['model']}\")\n",
        "    print(f\"      Base prediction: {pred['base']:.1f}\")\n",
        "    print(f\"      ENSO correction: {pred['corr']:.1f}\")\n",
        "    print(f\"      Final prediction: {pred['final']} typhoons\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "predict_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£4Ô∏è‚É£ Download Results"
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download comparison CSV\n",
        "files.download(f'{Config.OUTPUT_DIR}/model_comparison_improved.csv')\n",
        "\n",
        "# Download plot\n",
        "files.download(f'{Config.OUTPUT_DIR}/predictions_improved.png')\n",
        "\n",
        "print(\"‚úÖ Files downloaded!\")"
      ],
      "metadata": {
        "id": "download_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìù Notes on Flat Prediction Fix\n",
        "\n",
        "### Why predictions were flat:\n",
        "1. **High SVR epsilon** (0.1-0.5) ignored small variations\n",
        "2. **Too many features** (8) for sample size (37)\n",
        "3. **Over-regularization** from small data\n",
        "\n",
        "### How this notebook fixes it:\n",
        "1. **Lower epsilon** (0.001-0.1) forces SVR to capture variance\n",
        "2. **Feature selection** reduces to 4 best features\n",
        "3. **Leave-One-Out CV** gives reliable estimates\n",
        "4. **Bayesian models** handle uncertainty better\n",
        "\n",
        "### If predictions are still flat:\n",
        "- The signal in climate indices may be weak\n",
        "- Try different feature combinations\n",
        "- Consider ensemble averaging\n",
        "- Accept that some uncertainty is inherent with 37 samples"
      ],
      "metadata": {
        "id": "notes_cell"
      }
    }
  ]
}
