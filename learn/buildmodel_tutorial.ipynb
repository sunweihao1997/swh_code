{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jMQoFMqwq5fL"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuVLZ1Raq5fO"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\|\n",
        "[Quickstart](quickstart_tutorial.html) \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| **Build Model** \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Build the Neural Network\n",
        "========================\n",
        "\n",
        "Neural networks comprise of layers/modules that perform operations on\n",
        "data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace\n",
        "provides all the building blocks you need to build your own neural\n",
        "network. Every module in PyTorch subclasses the\n",
        "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "A neural network is a module itself that consists of other modules\n",
        "(layers). This nested structure allows for building and managing complex\n",
        "architectures easily.\n",
        "\n",
        "In the following sections, we\\'ll build a neural network to classify\n",
        "images in the FashionMNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_yXbGNUbq5fQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk-4qIbWq5fQ"
      },
      "source": [
        "Get Device for Training\n",
        "=======================\n",
        "\n",
        "We want to be able to train our model on a hardware accelerator like the\n",
        "GPU or MPS, if available. Let\\'s check to see if\n",
        "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) or\n",
        "[torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) are\n",
        "available, otherwise we use the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZTF-F8L4q5fQ",
        "outputId": "0a7e5509-cb2f-4fda-fe07-cf4851d75ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* MPS是苹果的框架\n"
      ],
      "metadata": {
        "id": "LA_i-dovw4_a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7bZnwHVq5fR"
      },
      "source": [
        "Define the Class\n",
        "================\n",
        "\n",
        "We define our neural network by subclassing `nn.Module`, and initialize\n",
        "the neural network layers in `__init__`. Every `nn.Module` subclass\n",
        "implements the operations on input data in the `forward` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "shDHGY5Qq5fR"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWiNQNmbq5fR"
      },
      "source": [
        "We create an instance of `NeuralNetwork`, and move it to the `device`,\n",
        "and print its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ytTJq9uAq5fR",
        "outputId": "eb93930f-9027-45e8-f4bb-cb7fc07aab35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITBq3sgGq5fR"
      },
      "source": [
        "To use the model, we pass it the input data. This executes the model\\'s\n",
        "`forward`, along with some [background\n",
        "operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
        "Do not call `model.forward()` directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0\n",
        "corresponding to each output of 10 raw predicted values for each class,\n",
        "and dim=1 corresponding to the individual values of each output. We get\n",
        "the prediction probabilities by passing it through an instance of the\n",
        "`nn.Softmax` module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cEsSkbLbq5fR",
        "outputId": "9e592c6e-de96-425e-d768-d92f65e172e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([0], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.state_dict().items():\n",
        "    print(f\"Layer: {name}\")\n",
        "    print(f\"Parameters: {param}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "bjR9pMKZ3OP8",
        "outputId": "38d5d434-2b8e-44c2-ccf3-54f0a7943a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: linear_relu_stack.0.weight\n",
            "Parameters: tensor([[-0.0049,  0.0128,  0.0318,  ...,  0.0173, -0.0204, -0.0351],\n",
            "        [ 0.0202,  0.0038,  0.0138,  ..., -0.0012,  0.0189,  0.0086],\n",
            "        [-0.0170,  0.0153,  0.0129,  ...,  0.0165,  0.0191, -0.0120],\n",
            "        ...,\n",
            "        [ 0.0020,  0.0241,  0.0249,  ...,  0.0232,  0.0273,  0.0239],\n",
            "        [-0.0303, -0.0126,  0.0168,  ..., -0.0135,  0.0193,  0.0053],\n",
            "        [ 0.0052, -0.0091, -0.0306,  ...,  0.0221, -0.0117, -0.0184]],\n",
            "       device='cuda:0')\n",
            "--------------------------------------------------\n",
            "Layer: linear_relu_stack.0.bias\n",
            "Parameters: tensor([ 2.6383e-02, -3.5446e-02,  1.3896e-02, -2.0674e-02,  1.9225e-02,\n",
            "        -3.2090e-02,  1.6844e-02, -1.7002e-03, -1.5262e-02,  8.9034e-03,\n",
            "         8.4785e-03, -2.5233e-02,  2.3913e-02, -2.5272e-02,  1.4886e-02,\n",
            "         4.0631e-04,  3.5307e-02, -2.6608e-02,  5.7709e-03,  2.6886e-02,\n",
            "        -3.2420e-03,  1.0908e-02,  3.0316e-02, -1.5200e-03, -2.5438e-02,\n",
            "         1.5165e-02, -8.4056e-03, -5.3396e-03, -1.7170e-02, -1.4438e-02,\n",
            "         2.6285e-02, -3.1989e-02,  2.7814e-02,  1.1010e-03, -1.8311e-02,\n",
            "         3.9917e-03,  4.7743e-04, -3.5964e-03,  1.1601e-02,  5.6703e-03,\n",
            "        -6.0352e-03, -4.0363e-03, -3.0127e-02,  1.4482e-02, -5.3511e-03,\n",
            "         3.3230e-02,  1.3060e-02, -2.9979e-02,  1.7302e-02, -1.9169e-02,\n",
            "         9.3710e-03, -8.8207e-03, -1.5913e-02, -2.5429e-02,  2.2674e-02,\n",
            "        -3.5540e-02,  3.1537e-02,  2.9435e-02, -4.5564e-03,  1.7193e-02,\n",
            "         2.1779e-02, -1.1140e-02,  2.5160e-02, -2.0704e-02, -3.3045e-02,\n",
            "        -9.6275e-03, -1.1185e-02, -6.1699e-03,  3.2285e-02,  2.0179e-03,\n",
            "        -8.5295e-03,  7.4215e-03, -1.2592e-02, -1.4208e-02,  1.7153e-02,\n",
            "        -1.5893e-02,  2.0482e-02, -3.1817e-02, -6.8905e-03, -5.4200e-03,\n",
            "         2.6726e-02,  2.5636e-02,  1.2971e-02, -1.6691e-02, -1.7899e-02,\n",
            "         2.7826e-02, -1.9517e-02, -9.1249e-03,  2.1767e-02,  1.0304e-02,\n",
            "        -1.2148e-02, -3.5025e-02,  8.1442e-03, -4.3025e-04,  2.8296e-02,\n",
            "         2.0018e-02,  1.5025e-02, -5.5177e-06,  1.8375e-03,  1.9874e-02,\n",
            "        -2.4170e-02, -4.9955e-03,  4.6619e-03, -2.4958e-02,  4.4715e-03,\n",
            "        -4.1932e-03, -3.5182e-02, -2.4304e-02,  1.0355e-02,  2.1313e-02,\n",
            "        -3.0923e-03,  1.7830e-02, -8.6262e-03, -2.8687e-02,  2.3209e-02,\n",
            "        -1.3018e-02, -2.1543e-02,  3.6080e-03,  2.6180e-02,  2.2982e-02,\n",
            "        -2.9379e-02, -2.9661e-02,  2.9136e-02,  3.0997e-02, -3.3909e-02,\n",
            "        -4.6828e-03, -1.1598e-02,  3.5012e-02, -2.8114e-02, -3.4648e-02,\n",
            "        -2.4897e-02, -2.5642e-02, -9.3736e-03, -2.1658e-02,  9.6708e-03,\n",
            "        -1.4040e-02,  3.7115e-03,  1.4236e-02,  1.1634e-02,  1.2258e-03,\n",
            "         2.0621e-02, -2.9875e-03,  2.3735e-02, -2.1216e-02,  2.0711e-02,\n",
            "        -8.5021e-03,  3.4988e-02, -2.5033e-02,  1.8246e-02, -5.7116e-03,\n",
            "         4.9097e-03,  2.0141e-02, -1.1619e-02, -2.0321e-02,  1.1307e-02,\n",
            "        -9.6341e-03,  3.0598e-02,  1.9124e-02, -1.3725e-02,  1.4972e-02,\n",
            "        -2.4878e-02, -3.3103e-03, -1.8989e-02, -3.3965e-02, -3.2065e-02,\n",
            "        -2.2815e-02,  2.6428e-02, -1.6694e-02, -5.1954e-03,  1.5263e-02,\n",
            "         1.1164e-02, -3.2510e-02, -7.7667e-03,  7.0770e-04,  1.6634e-02,\n",
            "        -3.5557e-02, -3.2154e-02,  3.3428e-02,  3.1108e-02, -6.9835e-03,\n",
            "        -2.0227e-03,  2.1816e-02, -3.3169e-02, -3.2017e-03, -3.4544e-02,\n",
            "        -1.8714e-02,  2.4492e-02,  2.4608e-02,  9.1737e-03,  1.8867e-02,\n",
            "         2.7330e-02,  3.4310e-02,  2.4531e-02, -4.4817e-03, -2.1489e-02,\n",
            "        -7.6299e-03, -1.0323e-02, -3.4822e-02, -1.0691e-03, -6.8013e-03,\n",
            "        -8.0834e-03,  1.8239e-02, -1.6478e-02,  2.5796e-02,  3.6709e-03,\n",
            "        -1.3707e-02,  2.7026e-02, -8.7479e-03, -3.2319e-02,  7.4595e-05,\n",
            "         3.3070e-03, -3.3943e-02, -1.4968e-03, -3.5209e-03, -1.5243e-02,\n",
            "        -2.7548e-02,  8.3738e-03,  2.8116e-02,  2.3964e-02,  7.2815e-03,\n",
            "        -2.9434e-02,  1.1962e-02, -3.1093e-02, -1.4286e-02,  2.7285e-02,\n",
            "         1.5562e-02, -1.9379e-02,  2.6908e-02, -3.9744e-03, -2.7439e-02,\n",
            "        -2.0764e-02,  2.9689e-02,  3.5065e-02,  5.9402e-03, -2.8671e-02,\n",
            "        -2.2274e-03,  2.0048e-02,  2.5498e-02, -1.3891e-02,  1.3087e-02,\n",
            "        -1.0159e-02, -1.0104e-02, -2.7198e-02, -1.5727e-02,  3.2208e-02,\n",
            "        -6.7859e-03, -1.6230e-03, -1.8494e-02,  1.9576e-02,  1.6036e-02,\n",
            "         3.1014e-02,  9.7451e-03,  1.0491e-02,  3.0829e-02, -2.0218e-02,\n",
            "         2.2822e-03,  3.0048e-02,  2.1271e-03, -1.9281e-02,  6.3530e-03,\n",
            "         4.0032e-03,  1.6775e-02,  3.0352e-02,  2.8285e-02, -9.5476e-03,\n",
            "        -2.6574e-02,  1.8447e-02,  2.5350e-02, -4.4296e-03, -1.5990e-02,\n",
            "        -1.4709e-02,  6.1229e-03,  2.0307e-03,  2.7044e-02,  1.3028e-02,\n",
            "         1.4826e-02, -3.8138e-03, -1.3364e-03,  2.6829e-02,  2.7156e-02,\n",
            "         3.0185e-03,  3.1835e-02, -5.7397e-03,  4.9029e-03,  1.1640e-02,\n",
            "        -1.3673e-02,  2.4971e-02,  7.1837e-03, -3.2724e-02, -1.6935e-02,\n",
            "        -8.2774e-05, -7.1668e-03, -1.4924e-02,  8.4444e-03, -3.2938e-02,\n",
            "        -1.7322e-03, -2.7946e-02, -3.5612e-02, -2.3279e-02,  2.6576e-02,\n",
            "        -1.2508e-02,  2.2199e-02, -3.2228e-02, -1.6752e-02,  9.4521e-03,\n",
            "         3.2665e-02, -3.4159e-02,  3.2525e-02, -6.1309e-03, -1.0032e-02,\n",
            "        -2.8871e-02, -2.5256e-02,  1.0732e-02, -1.4448e-02, -1.3242e-02,\n",
            "        -1.5369e-02,  1.3223e-02,  6.5719e-03, -1.6624e-02,  2.3409e-02,\n",
            "         6.7596e-03, -2.7985e-04, -6.5909e-03,  1.5644e-03, -4.5239e-03,\n",
            "         3.2791e-02, -1.8883e-03,  2.7249e-02,  1.4847e-02, -9.8156e-03,\n",
            "        -2.2415e-02,  2.2991e-02, -2.9306e-02, -2.7998e-03, -3.1566e-02,\n",
            "         1.2636e-02,  1.4532e-02, -1.1809e-02,  8.2016e-03,  1.8453e-02,\n",
            "        -2.4868e-02,  4.5499e-04,  2.5273e-02,  8.5254e-03,  2.1967e-02,\n",
            "         3.2952e-02, -2.8955e-02,  3.0969e-02,  7.3062e-03, -3.3386e-02,\n",
            "         9.8999e-03,  3.1317e-02,  1.9566e-02,  4.7299e-03, -2.3225e-02,\n",
            "         2.1054e-02,  1.0501e-02, -1.8018e-02, -1.7007e-02,  8.4233e-03,\n",
            "         3.3534e-02,  1.7638e-02, -2.5131e-03, -9.9059e-03,  2.6912e-02,\n",
            "        -1.1203e-02,  1.7728e-02,  2.6213e-02, -1.1212e-02,  3.3797e-02,\n",
            "         2.4059e-02,  2.5118e-02,  1.1426e-02, -2.9846e-02, -3.2186e-02,\n",
            "         3.2532e-02,  1.1345e-02,  3.3493e-02,  1.9682e-02, -2.9470e-02,\n",
            "        -3.4967e-02, -1.0444e-02,  1.2436e-03, -7.0819e-03, -4.7713e-03,\n",
            "        -1.8977e-02,  3.5806e-03, -2.5949e-02,  5.9921e-03, -1.0285e-03,\n",
            "         4.2463e-03,  6.9990e-03,  9.7541e-03, -6.0266e-03, -3.4268e-02,\n",
            "        -1.9923e-02,  1.8552e-02, -2.3508e-02,  2.4321e-02,  2.1205e-02,\n",
            "         1.2859e-02, -3.2590e-03, -7.1354e-03,  6.9141e-03,  2.6446e-02,\n",
            "        -1.3565e-02, -1.8993e-02, -1.3301e-02,  1.1120e-02, -1.1552e-02,\n",
            "        -9.1717e-03,  3.7957e-03,  2.6155e-02, -2.0009e-02,  7.6898e-03,\n",
            "        -2.1200e-02, -1.8049e-02,  2.4452e-02, -1.9725e-02, -1.0149e-02,\n",
            "        -2.9023e-02,  1.0458e-02,  2.9162e-02, -1.1630e-02, -1.0189e-02,\n",
            "         2.4603e-02, -5.3000e-04,  3.0209e-02, -2.7247e-02, -4.1989e-03,\n",
            "        -7.4207e-03, -9.2020e-03, -3.3371e-02,  1.3399e-02,  2.9761e-02,\n",
            "        -2.2205e-02,  2.5580e-02,  2.4816e-02, -3.3545e-02, -1.9268e-02,\n",
            "         1.8389e-02,  2.4892e-02, -1.7596e-02,  6.2264e-03, -3.2037e-02,\n",
            "         1.8958e-02,  1.3465e-02,  1.0256e-03, -2.8070e-02,  2.8938e-02,\n",
            "         3.3568e-03,  3.3083e-02, -1.0167e-02, -2.0179e-02,  2.3677e-02,\n",
            "         3.1317e-02,  2.1535e-02,  1.0423e-02,  1.3764e-02,  1.6688e-02,\n",
            "         3.3056e-02, -3.3679e-02, -2.4327e-02,  2.1778e-03, -1.1721e-02,\n",
            "        -2.9046e-02, -2.4013e-02,  3.2556e-02,  1.7493e-02, -1.6210e-02,\n",
            "         2.3772e-02, -3.5531e-02,  3.5530e-02, -3.0845e-02, -1.8548e-02,\n",
            "         2.2682e-02, -8.1850e-03,  2.0355e-02, -2.2702e-02,  2.7109e-02,\n",
            "         2.4767e-02,  3.3462e-02, -1.4793e-02, -7.8042e-03, -1.8603e-02,\n",
            "        -2.6863e-02,  1.8423e-02,  2.4604e-02, -2.8595e-02,  9.2305e-03,\n",
            "        -1.1296e-02, -3.1839e-02, -3.2350e-02, -3.5509e-02,  1.7655e-02,\n",
            "        -7.2238e-03,  3.2570e-03, -3.0081e-02,  3.2008e-02, -2.8409e-02,\n",
            "        -2.4651e-03,  1.6288e-02,  1.8927e-02,  1.9206e-02,  4.6924e-03,\n",
            "         6.8636e-03, -2.3629e-02,  9.2189e-03, -4.0084e-03, -1.7332e-02,\n",
            "         2.2400e-02, -2.9872e-02], device='cuda:0')\n",
            "--------------------------------------------------\n",
            "Layer: linear_relu_stack.2.weight\n",
            "Parameters: tensor([[-0.0033, -0.0269,  0.0004,  ...,  0.0198, -0.0133, -0.0082],\n",
            "        [ 0.0008,  0.0314, -0.0333,  ...,  0.0213, -0.0065, -0.0038],\n",
            "        [-0.0177,  0.0177,  0.0170,  ...,  0.0308,  0.0405,  0.0424],\n",
            "        ...,\n",
            "        [-0.0118, -0.0101,  0.0078,  ...,  0.0369,  0.0247, -0.0270],\n",
            "        [ 0.0183,  0.0023, -0.0416,  ...,  0.0057, -0.0241, -0.0240],\n",
            "        [ 0.0286, -0.0330, -0.0077,  ...,  0.0252,  0.0052, -0.0030]],\n",
            "       device='cuda:0')\n",
            "--------------------------------------------------\n",
            "Layer: linear_relu_stack.2.bias\n",
            "Parameters: tensor([-2.8445e-03,  3.7988e-03,  3.9454e-02, -2.8558e-02,  2.1481e-02,\n",
            "        -1.7355e-02, -2.7170e-02, -2.8599e-02,  1.8547e-02, -2.9284e-02,\n",
            "         2.8345e-02, -2.5210e-02,  1.5727e-04, -3.6334e-02,  2.2672e-02,\n",
            "        -1.3278e-02, -3.6678e-02, -2.5956e-02, -4.1569e-02,  3.6941e-02,\n",
            "        -3.5731e-02, -4.9819e-03,  2.7835e-02,  3.9024e-02,  1.5201e-02,\n",
            "         4.2559e-02,  3.5224e-02, -1.4516e-03, -2.6995e-03, -3.4019e-03,\n",
            "        -2.9539e-02,  3.3226e-02, -3.7276e-02, -2.7513e-02, -1.5450e-02,\n",
            "        -1.6387e-02, -9.5177e-03,  2.2422e-02,  4.7897e-03,  3.6317e-02,\n",
            "         9.6624e-03, -1.0503e-02,  5.4140e-03,  3.4742e-02,  3.4701e-02,\n",
            "         1.8628e-03, -9.0640e-03,  1.5311e-02,  3.7537e-02,  3.4114e-02,\n",
            "         1.1290e-02, -1.0917e-02,  3.5126e-03, -3.9123e-02, -3.0362e-02,\n",
            "        -2.6682e-03, -3.3288e-02,  3.7721e-02,  1.6940e-02, -3.4176e-02,\n",
            "        -1.3510e-02,  3.2822e-02,  1.0917e-02,  4.0591e-03, -2.3634e-02,\n",
            "         2.7309e-02, -1.2258e-02, -6.0829e-03,  3.5731e-02, -1.9810e-02,\n",
            "         4.1616e-02,  2.5474e-02, -2.8273e-02, -2.1679e-02, -4.5394e-03,\n",
            "         9.0588e-03, -2.2649e-02,  3.6314e-02, -1.0279e-02, -2.5552e-02,\n",
            "        -1.0770e-02,  4.1230e-03,  3.8928e-02,  2.6721e-03,  2.4686e-02,\n",
            "        -1.2885e-02,  1.9049e-02, -4.0976e-02,  8.5131e-03, -3.8960e-02,\n",
            "        -3.1245e-02, -3.7249e-02,  1.6698e-03,  3.8131e-02, -3.6720e-02,\n",
            "         4.4094e-02,  3.6498e-03,  2.3149e-02, -2.9024e-02, -1.1391e-02,\n",
            "        -2.4281e-02,  3.2623e-02, -1.4508e-02, -2.9254e-02, -8.6677e-03,\n",
            "         2.8993e-02,  3.5235e-02, -1.1199e-02,  2.5043e-02, -2.7833e-02,\n",
            "        -3.3957e-02,  1.7099e-02,  2.7896e-03, -1.7634e-02, -3.0290e-02,\n",
            "         1.1280e-02,  1.5038e-02, -1.9587e-02, -1.9939e-02, -2.7173e-02,\n",
            "         1.1849e-02, -2.1606e-02,  1.7284e-02, -7.5748e-03, -3.0723e-03,\n",
            "        -3.4526e-02,  3.9180e-02,  1.8106e-02,  1.6383e-02,  3.7552e-02,\n",
            "        -3.0897e-02,  4.3904e-02, -1.9704e-02, -2.4651e-02, -3.1513e-02,\n",
            "         4.3025e-02,  3.4580e-03, -1.5678e-02, -1.9081e-02,  2.9894e-02,\n",
            "         1.8053e-02, -2.9459e-02, -1.7462e-02,  1.6728e-02, -1.4182e-02,\n",
            "         4.0046e-02, -1.2816e-02, -9.9388e-04, -7.4996e-03,  2.9808e-02,\n",
            "        -3.5412e-02,  9.2949e-03, -8.1219e-03,  4.2345e-02,  1.6681e-02,\n",
            "         1.0656e-02, -3.9983e-02,  4.3904e-02,  2.1635e-02,  2.2520e-02,\n",
            "         1.9257e-03,  1.9105e-02, -7.1310e-03,  3.6218e-03,  2.2359e-02,\n",
            "        -9.8383e-03,  1.6810e-02, -1.5009e-02, -4.3072e-02,  1.7364e-02,\n",
            "         3.2048e-02, -6.0488e-03,  1.6140e-02,  3.6297e-02, -3.9000e-03,\n",
            "        -5.8690e-03,  2.0161e-02,  5.5810e-03, -3.7765e-02,  2.6405e-02,\n",
            "        -3.4931e-02,  4.0622e-02, -8.4487e-03, -2.7566e-02,  3.9882e-02,\n",
            "        -3.6654e-03,  3.2665e-02,  3.3928e-02, -3.8011e-02, -3.0752e-02,\n",
            "        -3.1251e-02, -4.3321e-02, -2.4856e-02, -2.2816e-02,  3.1174e-02,\n",
            "         2.0388e-02, -1.1253e-02, -1.4584e-02,  1.0364e-02,  1.5359e-02,\n",
            "        -4.3883e-02, -1.7336e-02, -4.2217e-02,  2.7048e-02, -1.5999e-02,\n",
            "        -3.5982e-02,  4.2256e-02, -2.5727e-02, -2.0005e-02, -2.9283e-02,\n",
            "         4.3969e-02,  2.8944e-02,  4.0798e-02, -2.0912e-02, -3.9198e-02,\n",
            "        -4.1675e-02,  2.9969e-02, -2.0796e-02, -1.5513e-02, -3.7323e-02,\n",
            "         2.9427e-02,  3.2442e-02, -8.9680e-03, -1.7560e-02,  6.5061e-03,\n",
            "         4.5664e-03, -2.8423e-02,  1.0164e-02,  2.3228e-02,  2.2431e-02,\n",
            "         3.7519e-03,  1.7593e-02, -3.1188e-02, -3.0816e-02,  9.8332e-03,\n",
            "         2.1643e-02, -1.0202e-02, -1.2779e-02, -4.0942e-02,  2.5362e-02,\n",
            "        -2.0285e-02, -3.6266e-02,  4.3919e-02, -7.7132e-03, -1.9103e-02,\n",
            "         3.6163e-02,  2.6499e-02, -2.9531e-02,  1.3861e-02, -3.4765e-02,\n",
            "         3.6142e-02, -5.1996e-03,  3.5976e-02,  2.1156e-02,  1.5972e-02,\n",
            "         3.8933e-02,  9.0412e-03,  3.4712e-02,  1.6259e-02,  1.2239e-02,\n",
            "         3.7561e-03,  3.7190e-02,  2.4743e-02,  5.0698e-03,  9.8296e-03,\n",
            "         2.9373e-02,  3.5887e-02, -1.5546e-02, -3.1812e-02, -4.0206e-02,\n",
            "         2.0708e-02, -1.8104e-02, -1.2972e-03, -2.5008e-02, -3.7397e-02,\n",
            "         8.7514e-04,  2.3403e-02, -1.2780e-02,  1.3438e-02, -4.4012e-02,\n",
            "        -4.0549e-02, -2.4440e-02,  1.5403e-02, -2.5363e-02,  1.6382e-02,\n",
            "         4.0513e-02,  1.8107e-02, -3.1609e-02, -3.3765e-02, -9.6121e-03,\n",
            "        -2.2462e-02, -3.3786e-02, -3.7303e-02,  1.2513e-02,  2.3927e-03,\n",
            "        -3.8507e-02,  3.4918e-02, -3.5715e-02, -2.1691e-03, -3.2226e-02,\n",
            "        -1.2091e-02,  9.6976e-03, -3.4388e-02, -4.3930e-02, -3.4245e-02,\n",
            "        -1.1961e-02,  3.2797e-02,  4.6384e-03, -2.2985e-02,  1.3164e-02,\n",
            "        -3.1609e-02, -3.3003e-02,  9.2675e-03, -2.6511e-03,  3.3775e-02,\n",
            "        -2.4821e-02,  3.4140e-02, -3.8068e-02,  2.1050e-02, -1.9259e-02,\n",
            "        -2.1715e-02, -3.4642e-02, -6.0422e-03, -2.8861e-02, -3.1997e-02,\n",
            "        -3.3048e-02, -1.6515e-02,  2.2887e-02, -6.6892e-03, -5.3416e-03,\n",
            "         2.4821e-02, -4.1041e-02, -2.1254e-03, -3.3672e-02, -3.0689e-02,\n",
            "         2.9844e-02,  1.8192e-02, -1.6872e-03, -3.0931e-02,  1.4502e-02,\n",
            "         1.2609e-02, -3.2223e-02,  4.0832e-03,  1.1556e-02, -4.0027e-02,\n",
            "        -2.7755e-02, -2.7393e-02,  2.0623e-02, -4.1420e-02,  5.8039e-03,\n",
            "        -3.3735e-02, -4.1074e-02, -4.4073e-02,  6.1362e-03,  2.6116e-02,\n",
            "        -2.1375e-02,  3.2388e-02,  3.6197e-02, -3.5125e-02,  3.7187e-02,\n",
            "         2.9085e-02,  8.0111e-03, -1.1953e-02,  3.1154e-03,  4.1732e-02,\n",
            "        -2.3675e-02,  8.7176e-03, -1.1711e-02,  3.8261e-02, -4.3604e-02,\n",
            "        -3.7944e-02,  3.8123e-02, -9.2370e-03,  1.0251e-02,  2.8676e-02,\n",
            "         1.6641e-02, -1.4286e-02,  3.2004e-02, -4.2400e-02, -3.4539e-02,\n",
            "         2.2722e-02, -9.0963e-03,  3.6911e-02, -3.0071e-02,  3.5396e-02,\n",
            "         3.5157e-02,  2.7164e-03,  2.9197e-02, -1.7335e-02, -1.1401e-02,\n",
            "        -9.5092e-03,  1.0066e-03,  3.1125e-02, -1.7839e-02, -2.3284e-02,\n",
            "        -3.0665e-02, -1.0042e-02,  1.5031e-02, -2.7223e-02,  2.9537e-02,\n",
            "         7.4241e-04, -1.1819e-02, -4.2442e-02, -9.4071e-03,  2.9213e-02,\n",
            "         5.2873e-03, -4.0941e-02,  2.3535e-02, -1.7285e-02,  4.1943e-02,\n",
            "         1.1231e-02, -4.7710e-05, -3.1548e-02,  3.0186e-02,  3.3677e-02,\n",
            "         2.2248e-02,  2.4856e-02,  3.2262e-02, -2.5796e-02, -2.3217e-02,\n",
            "         1.0856e-02, -2.4435e-02, -1.4685e-02,  2.8805e-02,  8.0024e-03,\n",
            "         4.4168e-02, -4.3360e-02,  1.8390e-02,  2.7855e-02, -1.1009e-04,\n",
            "         3.4707e-02,  7.0756e-04, -1.7169e-02, -1.2585e-02, -2.5318e-02,\n",
            "        -4.5572e-03,  1.6196e-02,  1.8790e-02,  2.7250e-03, -4.1087e-02,\n",
            "        -5.9153e-03,  1.2978e-02,  4.3052e-02,  1.0996e-02, -2.3122e-02,\n",
            "        -3.2015e-02,  4.3627e-02, -2.0492e-02,  1.1867e-02, -3.9340e-02,\n",
            "        -4.1833e-02, -9.7828e-03, -3.8571e-02,  3.6579e-02,  3.5326e-02,\n",
            "         3.3036e-02, -3.2244e-02, -1.3292e-02,  2.4801e-02,  3.3933e-02,\n",
            "        -9.1973e-03, -3.1473e-03, -1.5923e-02,  2.6251e-02, -1.0932e-02,\n",
            "         2.9479e-02,  4.2941e-02, -4.0121e-02,  3.3848e-02,  3.3375e-03,\n",
            "        -3.2528e-02, -4.4078e-02,  4.3890e-02,  4.3300e-02,  4.2888e-02,\n",
            "         1.2023e-02,  4.0207e-02,  5.0545e-03,  2.0530e-02,  4.4183e-02,\n",
            "        -1.2889e-02,  1.9028e-02, -2.2154e-02, -1.4550e-02,  3.9741e-02,\n",
            "        -2.7579e-02,  2.6039e-02, -4.3665e-02, -5.2992e-03, -2.6411e-02,\n",
            "        -2.3515e-02, -2.7168e-02, -1.5157e-02,  2.3100e-03, -3.8448e-03,\n",
            "         4.3210e-02, -2.4468e-02, -2.8252e-02, -3.4829e-02, -2.1524e-02,\n",
            "         2.0838e-02,  7.0821e-03,  7.0098e-03, -3.4777e-02,  2.8224e-02,\n",
            "        -1.8778e-02, -3.0238e-02,  2.7872e-02, -2.6305e-02, -4.7223e-03,\n",
            "        -2.7740e-02,  1.6293e-03], device='cuda:0')\n",
            "--------------------------------------------------\n",
            "Layer: linear_relu_stack.4.weight\n",
            "Parameters: tensor([[-0.0376, -0.0117,  0.0379,  ..., -0.0239, -0.0375,  0.0214],\n",
            "        [-0.0109,  0.0310, -0.0280,  ...,  0.0247,  0.0251,  0.0303],\n",
            "        [-0.0282, -0.0194,  0.0386,  ..., -0.0040,  0.0284,  0.0092],\n",
            "        ...,\n",
            "        [-0.0032,  0.0376,  0.0365,  ...,  0.0277,  0.0086, -0.0399],\n",
            "        [ 0.0397, -0.0219, -0.0276,  ..., -0.0314, -0.0432,  0.0242],\n",
            "        [ 0.0241, -0.0081,  0.0086,  ...,  0.0141, -0.0396,  0.0229]],\n",
            "       device='cuda:0')\n",
            "--------------------------------------------------\n",
            "Layer: linear_relu_stack.4.bias\n",
            "Parameters: tensor([ 0.0378, -0.0311, -0.0342, -0.0291, -0.0278, -0.0022, -0.0351, -0.0264,\n",
            "         0.0166, -0.0092], device='cuda:0')\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EDSTg4zq5fR"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FvkdGqOq5fS"
      },
      "source": [
        "Model Layers\n",
        "============\n",
        "\n",
        "Let\\'s break down the layers in the FashionMNIST model. To illustrate\n",
        "it, we will take a sample minibatch of 3 images of size 28x28 and see\n",
        "what happens to it as we pass it through the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LDWy84Q1q5fS",
        "outputId": "ad6f1e06-6327-4bc4-ee08-46b6b6eb0911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUafpVmRq5fS"
      },
      "source": [
        "nn.Flatten\n",
        "==========\n",
        "\n",
        "We initialize the\n",
        "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
        "layer to convert each 2D 28x28 image into a contiguous array of 784\n",
        "pixel values ( the minibatch dimension (at dim=0) is maintained).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fALcCRE-q5fS",
        "outputId": "7fffbcd0-d596-4fea-85a3-44ef373ee51c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Flatten()是从第二维开始展开的！"
      ],
      "metadata": {
        "id": "nPRLm1Mk6lQa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jclwja8aq5fS"
      },
      "source": [
        "nn.Linear\n",
        "=========\n",
        "\n",
        "The [linear\n",
        "layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "is a module that applies a linear transformation on the input using its\n",
        "stored weights and biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z8pXb7whq5fS",
        "outputId": "c24b3926-262f-468b-924d-a2471620de27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nac8PPPoq5fS"
      },
      "source": [
        "nn.ReLU\n",
        "=======\n",
        "\n",
        "Non-linear activations are what create the complex mappings between the\n",
        "model\\'s inputs and outputs. They are applied after linear\n",
        "transformations to introduce *nonlinearity*, helping neural networks\n",
        "learn a wide variety of phenomena.\n",
        "\n",
        "In this model, we use\n",
        "[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
        "between our linear layers, but there\\'s other activations to introduce\n",
        "non-linearity in your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RpOoHYycq5fT",
        "outputId": "48916761-692a-4bae-ac83-225726215a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[-0.5607, -0.1083,  0.0623,  0.3736,  0.2574,  0.1707, -0.1046, -0.5103,\n",
            "          0.1268, -0.0930,  0.2521,  0.1772, -0.0455,  0.1547,  0.1576,  0.0492,\n",
            "          0.0445,  0.0026,  0.5106, -0.0332],\n",
            "        [-0.5189,  0.0398,  0.1567,  0.3442,  0.2293,  0.4436, -0.2753, -0.9907,\n",
            "          0.4714, -0.1631, -0.2132,  0.1636,  0.2717,  0.1548,  0.3589, -0.1715,\n",
            "         -0.2148,  0.3832,  0.4935, -0.3987],\n",
            "        [-0.5771, -0.0132,  0.0528,  0.6429,  0.0387,  0.5962, -0.3136, -0.4916,\n",
            "          0.7058, -0.2945,  0.0137,  0.1006, -0.0687, -0.1460,  0.7408,  0.0595,\n",
            "         -0.3037, -0.0662,  0.3911, -0.3258]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000, 0.0000, 0.0623, 0.3736, 0.2574, 0.1707, 0.0000, 0.0000, 0.1268,\n",
            "         0.0000, 0.2521, 0.1772, 0.0000, 0.1547, 0.1576, 0.0492, 0.0445, 0.0026,\n",
            "         0.5106, 0.0000],\n",
            "        [0.0000, 0.0398, 0.1567, 0.3442, 0.2293, 0.4436, 0.0000, 0.0000, 0.4714,\n",
            "         0.0000, 0.0000, 0.1636, 0.2717, 0.1548, 0.3589, 0.0000, 0.0000, 0.3832,\n",
            "         0.4935, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0528, 0.6429, 0.0387, 0.5962, 0.0000, 0.0000, 0.7058,\n",
            "         0.0000, 0.0137, 0.1006, 0.0000, 0.0000, 0.7408, 0.0595, 0.0000, 0.0000,\n",
            "         0.3911, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRdJl6Csq5fT"
      },
      "source": [
        "nn.Sequential\n",
        "=============\n",
        "\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
        "is an ordered container of modules. The data is passed through all the\n",
        "modules in the same order as defined. You can use sequential containers\n",
        "to put together a quick network like `seq_modules`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2EiNV3buq5fT"
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* nn.Linear(20, 10) 前面是输入的维度，后面是输出的维度"
      ],
      "metadata": {
        "id": "mT1x0piN7cQh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNyxu77wq5fT"
      },
      "source": [
        "nn.Softmax\n",
        "==========\n",
        "\n",
        "The last linear layer of the neural network returns [logits]{.title-ref}\n",
        "- raw values in \\[-infty, infty\\] - which are passed to the\n",
        "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
        "module. The logits are scaled to values \\[0, 1\\] representing the\n",
        "model\\'s predicted probabilities for each class. `dim` parameter\n",
        "indicates the dimension along which the values must sum to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba-pBBr_q5fT"
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzlDOdrq5fT"
      },
      "source": [
        "Model Parameters\n",
        "================\n",
        "\n",
        "Many layers inside a neural network are *parameterized*, i.e. have\n",
        "associated weights and biases that are optimized during training.\n",
        "Subclassing `nn.Module` automatically tracks all fields defined inside\n",
        "your model object, and makes all parameters accessible using your\n",
        "model\\'s `parameters()` or `named_parameters()` methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and\n",
        "a preview of its values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CFDvtBOq5fT"
      },
      "outputs": [],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIE2mLHUq5fT"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBZJraO7q5fT"
      },
      "source": [
        "Further Reading\n",
        "===============\n",
        "\n",
        "-   [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}